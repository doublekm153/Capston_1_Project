{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "\u001b[A                                                                  \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                                           \n",
      "\n",
      "chunk:   1%|▏         | 27/1879 [19:09<21:54:17, 42.58s/it, now=None]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in D:/video-summarization-master/videos/new/temp.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                     \n",
      "\u001b[A                                                                  \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                                            \n",
      "\n",
      "chunk:   1%|▏         | 27/1879 [19:10<21:54:46, 42.60s/it, now=None]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/video-summarization-master/videos/new/text\\\\1234.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m text_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(video_filename)[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m text_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(text_dir, text_filename)\n\u001b[1;32m---> 84\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(text_path, \u001b[39m\"\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m text_file:\n\u001b[0;32m     85\u001b[0m     \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m response:\n\u001b[0;32m     86\u001b[0m         text_file\u001b[39m.\u001b[39mwrite(result\u001b[39m.\u001b[39malternatives[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranscript)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/video-summarization-master/videos/new/text\\\\1234.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "from moviepy.editor import *\n",
    "from pydub import AudioSegment\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/user/Desktop/graduate-388812-57af2b042054.json\"\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    temp_audio_path = \"D:/video-summarization-master/videos/new/temp.wav\"  \n",
    "    audio.write_audiofile(temp_audio_path, codec='pcm_s16le')  \n",
    "\n",
    "    audio_segment = AudioSegment.from_wav(temp_audio_path)\n",
    "    mono_audio = audio_segment.set_channels(1)\n",
    "    mono_audio.export(audio_path, format=\"wav\")\n",
    "\n",
    "    os.remove(temp_audio_path)\n",
    "\n",
    "def split_audio_into_chunks(audio_path, chunk_length=5000):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    audio_chunks = []\n",
    "\n",
    "    if len(audio) <= chunk_length:\n",
    "        return [audio_path]\n",
    "\n",
    "    for i, chunk in enumerate(audio[::chunk_length]):\n",
    "        chunk_path = f\"{audio_path[:-4]}_chunk{i}.wav\"\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "        audio_chunks.append(chunk_path)\n",
    "\n",
    "    return audio_chunks\n",
    "\n",
    "def transcribe_video(audio_path):\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    audio_chunks = split_audio_into_chunks(audio_path)\n",
    "\n",
    "    final_response = []\n",
    "\n",
    "    for chunk_path in audio_chunks:\n",
    "        with io.open(chunk_path, \"rb\") as audio_file:\n",
    "            content = audio_file.read()\n",
    "\n",
    "        audio = speech.RecognitionAudio(content=content)\n",
    "        config = speech.RecognitionConfig(\n",
    "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "            sample_rate_hertz=44100,\n",
    "            language_code=\"en-US\",\n",
    "            enable_automatic_punctuation=True,\n",
    "            enable_word_time_offsets=True,\n",
    "        )\n",
    "\n",
    "        operation = client.long_running_recognize(config=config, audio=audio)\n",
    "        response = operation.result(timeout=500)\n",
    "\n",
    "        final_response.extend(response.results)\n",
    "\n",
    "        os.remove(chunk_path)\n",
    "\n",
    "    return final_response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_dir = \"D:/video-summarization-master/videos/new/data2\"\n",
    "    audio_dir = \"D:/video-summarization-master/videos/new/wav\"\n",
    "    text_dir = \"D:/video-summarization-master/videos/new/text\"\n",
    "\n",
    "    video_filenames = [f for f in os.listdir(video_dir) if f.endswith(\".mp4\")]\n",
    "\n",
    "    for video_filename in video_filenames:\n",
    "        video_path = os.path.join(video_dir, video_filename)\n",
    "\n",
    "        audio_filename = f\"{os.path.splitext(video_filename)[0]}.wav\"\n",
    "        audio_path = os.path.join(audio_dir, audio_filename)\n",
    "\n",
    "        extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "        response = transcribe_video(audio_path)\n",
    "\n",
    "        text_filename = f\"{os.path.splitext(video_filename)[0]}.txt\"\n",
    "        text_path = os.path.join(text_dir, text_filename)\n",
    "\n",
    "        with open(text_path, \"w\") as text_file:\n",
    "            for result in response:\n",
    "                text_file.write(result.alternatives[0].transcript)\n",
    "                text_file.write(\"\\n\")\n",
    "                text_file.write(\"Confidence: {}\\n\".format(result.alternatives[0].confidence))\n",
    "                text_file.write(\"\\n\")\n",
    "\n",
    "                for word in result.alternatives[0].words:\n",
    "                    text_file.write(\"Word: {}\\n\".format(word.word))\n",
    "                    text_file.write(\"Start time: {} seconds\\n\".format(word.start_time.total_seconds()))\n",
    "                    text_file.write(\"End time: {} seconds\\n\".format(word.end_time.total_seconds()))\n",
    "\n",
    "                    text_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m words \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([interval[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m interval \u001b[39min\u001b[39;00m word_intervals])\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m - 시작:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m초, 끝:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m초\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m:]\n\u001b[0;32m     26\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(words)\n\u001b[1;32m---> 27\u001b[0m sentences \u001b[39m=\u001b[39m kss\u001b[39m.\u001b[39;49msplit_sentences(text)\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sentences) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     30\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_modules\\sentences\\split_sentences.py:66\u001b[0m, in \u001b[0;36msplit_sentences\u001b[1;34m(text, backend, num_workers, strip, ignores)\u001b[0m\n\u001b[0;32m     63\u001b[0m _preprocessor \u001b[39m=\u001b[39m preprocessors[ignores_tuple]\n\u001b[0;32m     64\u001b[0m _postprocessor \u001b[39m=\u001b[39m postprocessors[ignores_tuple]\n\u001b[1;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m _run_job(\n\u001b[0;32m     67\u001b[0m     func\u001b[39m=\u001b[39;49mpartial(\n\u001b[0;32m     68\u001b[0m         _split_sentences,\n\u001b[0;32m     69\u001b[0m         backend\u001b[39m=\u001b[39;49mbackend,\n\u001b[0;32m     70\u001b[0m         strip\u001b[39m=\u001b[39;49mstrip,\n\u001b[0;32m     71\u001b[0m         preprocessor\u001b[39m=\u001b[39;49m_preprocessor,\n\u001b[0;32m     72\u001b[0m         postprocessor\u001b[39m=\u001b[39;49m_postprocessor,\n\u001b[0;32m     73\u001b[0m     ),\n\u001b[0;32m     74\u001b[0m     inputs\u001b[39m=\u001b[39;49mtext,\n\u001b[0;32m     75\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m     76\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_utils\\multiprocessing.py:26\u001b[0m, in \u001b[0;36m_run_job\u001b[1;34m(func, inputs, num_workers)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m num_workers \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m         output \u001b[39m=\u001b[39m func(inputs)\n\u001b[0;32m     27\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         output \u001b[39m=\u001b[39m [func(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_modules\\sentences\\split_sentences.py:108\u001b[0m, in \u001b[0;36m_split_sentences\u001b[1;34m(text, backend, strip, postprocess, recursion, preprocessor, postprocessor)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    107\u001b[0m     backup_sentence \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mbackup(text)\n\u001b[1;32m--> 108\u001b[0m     morphemes \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mpos(backup_sentence, drop_space\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    109\u001b[0m     syllables \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mpreprocess(morphemes)\n\u001b[0;32m    110\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], Syllable):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_modules\\morphemes\\analyzers.py:64\u001b[0m, in \u001b[0;36mPecabAnalyzer.pos\u001b[1;34m(self, text, drop_space)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, drop_space: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m     54\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m    Get pos information.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m        List[Tuple[str, str]]: output of analysis.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_analyzer\u001b[39m.\u001b[39;49mpos(text)\n\u001b[0;32m     65\u001b[0m     output \u001b[39m=\u001b[39m _preserve_space(text, output, spaces\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\v\u001b[39;00m\u001b[39m\\f\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m drop_space:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_pecab.py:45\u001b[0m, in \u001b[0;36mPeCab.pos\u001b[1;34m(self, text, drop_space)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, drop_space: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     tokenization_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(text)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     47\u001b[0m         (token, pos)\n\u001b[0;32m     48\u001b[0m         \u001b[39mfor\u001b[39;00m token, pos \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[39mor\u001b[39;00m (drop_space \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\f\u001b[39;00m\u001b[39m\\v\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_pecab.py:19\u001b[0m, in \u001b[0;36mPeCab._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mset_input(text)\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mincrement_token():\n\u001b[0;32m     20\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     token_attributes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mtoken_attributes\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:292\u001b[0m, in \u001b[0;36mTokenizer.increment_token\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend:\n\u001b[0;32m    291\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse()\n\u001b[0;32m    294\u001b[0m token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpending\u001b[39m.\u001b[39mpop()\n\u001b[0;32m    295\u001b[0m length \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mlength\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:380\u001b[0m, in \u001b[0;36mTokenizer.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    379\u001b[0m surface \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mslice_get(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos, pos_ahead \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 380\u001b[0m data_entries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_entries(surface)\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m data_entries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[39mfor\u001b[39;00m data_entry \u001b[39min\u001b[39;00m data_entries\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:58\u001b[0m, in \u001b[0;36mTokenizer.create_entries\u001b[1;34m(self, surface)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_entries\u001b[39m(\u001b[39mself\u001b[39m, surface):\n\u001b[0;32m     57\u001b[0m     data_entries \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 58\u001b[0m     word_ref_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mknown_dict[surface]\n\u001b[0;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m word_ref_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_datrie.py:230\u001b[0m, in \u001b[0;36mDoubleArrayTrie.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 230\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exact_match_search(key)\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value_from_table(idx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kss\n",
    "import re\n",
    "\n",
    "input_folder_path = \"D:\\\\video-summarization-master\\\\videos\\\\time_text\"\n",
    "output_folder_path = \"D:\\\\video-summarization-master\\\\videos\\\\ssk_text\"\n",
    "\n",
    "# 입력 폴더 내 모든 파일 확인\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    # .txt 파일인 경우만 처리\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "        output_file_path = os.path.join(output_folder_path, filename)\n",
    "\n",
    "        # 워드별 구분된 입력 파일을 문장별 구분된 출력 파일로 변환\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            content_lines = input_file.readlines()\n",
    "            word_intervals = []\n",
    "\n",
    "            # 각 단어의 시작 시간 기록\n",
    "            for line in content_lines:\n",
    "                word_start_time = float(line.split(' - 시작: ')[1].split('초')[0])\n",
    "                word_intervals.append((line.strip(), word_start_time))\n",
    "\n",
    "            words = ' '.join([interval[0] for interval in word_intervals]).replace(\" - 시작:\", '').replace(\"초, 끝:\", '').split(\"초\")[1:]\n",
    "            text = ' '.join(words)\n",
    "            sentences = kss.split_sentences(text)\n",
    "\n",
    "            if len(sentences) == 0:\n",
    "                continue\n",
    "\n",
    "            # 문장 구성 시작점 찾기\n",
    "            sentence_start_times = [word_intervals[0][1]]\n",
    "            words_index = 0\n",
    "\n",
    "            for sentence in sentences[:-1]:\n",
    "                words_index += len(sentence.split()) - 1\n",
    "                if words_index < len(word_intervals) - 1:\n",
    "                    sentence_start_times.append(word_intervals[words_index+1][1])\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # 문장 생성 및 출력\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if i < len(sentence_start_times):\n",
    "                    start_time = sentence_start_times[i]\n",
    "                else:\n",
    "                    start_time = word_intervals[-1][1]\n",
    "\n",
    "                if i+1 < len(sentence_start_times):\n",
    "                    end_time = sentence_start_times[i+1]\n",
    "                else:\n",
    "                    end_time = word_intervals[-1][1]\n",
    "\n",
    "                output_line = f\"{sentence} - 시작:{start_time:.1f}초, 끝:{end_time:.1f}초\"\n",
    "                output_file.write(output_line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m words \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([interval[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m interval \u001b[39min\u001b[39;00m word_intervals])\n\u001b[0;32m     27\u001b[0m text \u001b[39m=\u001b[39m words\n\u001b[1;32m---> 28\u001b[0m sentences \u001b[39m=\u001b[39m kss\u001b[39m.\u001b[39;49msplit_sentences(text)\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sentences) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     31\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_modules\\sentences\\split_sentences.py:66\u001b[0m, in \u001b[0;36msplit_sentences\u001b[1;34m(text, backend, num_workers, strip, ignores)\u001b[0m\n\u001b[0;32m     63\u001b[0m _preprocessor \u001b[39m=\u001b[39m preprocessors[ignores_tuple]\n\u001b[0;32m     64\u001b[0m _postprocessor \u001b[39m=\u001b[39m postprocessors[ignores_tuple]\n\u001b[1;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m _run_job(\n\u001b[0;32m     67\u001b[0m     func\u001b[39m=\u001b[39;49mpartial(\n\u001b[0;32m     68\u001b[0m         _split_sentences,\n\u001b[0;32m     69\u001b[0m         backend\u001b[39m=\u001b[39;49mbackend,\n\u001b[0;32m     70\u001b[0m         strip\u001b[39m=\u001b[39;49mstrip,\n\u001b[0;32m     71\u001b[0m         preprocessor\u001b[39m=\u001b[39;49m_preprocessor,\n\u001b[0;32m     72\u001b[0m         postprocessor\u001b[39m=\u001b[39;49m_postprocessor,\n\u001b[0;32m     73\u001b[0m     ),\n\u001b[0;32m     74\u001b[0m     inputs\u001b[39m=\u001b[39;49mtext,\n\u001b[0;32m     75\u001b[0m     num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[0;32m     76\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_utils\\multiprocessing.py:26\u001b[0m, in \u001b[0;36m_run_job\u001b[1;34m(func, inputs, num_workers)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m num_workers \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m         output \u001b[39m=\u001b[39m func(inputs)\n\u001b[0;32m     27\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         output \u001b[39m=\u001b[39m [func(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_modules\\sentences\\split_sentences.py:108\u001b[0m, in \u001b[0;36m_split_sentences\u001b[1;34m(text, backend, strip, postprocess, recursion, preprocessor, postprocessor)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    107\u001b[0m     backup_sentence \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mbackup(text)\n\u001b[1;32m--> 108\u001b[0m     morphemes \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mpos(backup_sentence, drop_space\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    109\u001b[0m     syllables \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mpreprocess(morphemes)\n\u001b[0;32m    110\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], Syllable):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\kss\\_modules\\morphemes\\analyzers.py:64\u001b[0m, in \u001b[0;36mPecabAnalyzer.pos\u001b[1;34m(self, text, drop_space)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, drop_space: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m     54\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m    Get pos information.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m        List[Tuple[str, str]]: output of analysis.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_analyzer\u001b[39m.\u001b[39;49mpos(text)\n\u001b[0;32m     65\u001b[0m     output \u001b[39m=\u001b[39m _preserve_space(text, output, spaces\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\v\u001b[39;00m\u001b[39m\\f\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m drop_space:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_pecab.py:45\u001b[0m, in \u001b[0;36mPeCab.pos\u001b[1;34m(self, text, drop_space)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, drop_space: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     tokenization_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(text)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     47\u001b[0m         (token, pos)\n\u001b[0;32m     48\u001b[0m         \u001b[39mfor\u001b[39;00m token, pos \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[39mor\u001b[39;00m (drop_space \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\f\u001b[39;00m\u001b[39m\\v\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_pecab.py:19\u001b[0m, in \u001b[0;36mPeCab._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mset_input(text)\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mincrement_token():\n\u001b[0;32m     20\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     token_attributes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mtoken_attributes\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:292\u001b[0m, in \u001b[0;36mTokenizer.increment_token\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend:\n\u001b[0;32m    291\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse()\n\u001b[0;32m    294\u001b[0m token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpending\u001b[39m.\u001b[39mpop()\n\u001b[0;32m    295\u001b[0m length \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mlength\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:384\u001b[0m, in \u001b[0;36mTokenizer.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m data_entries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[39mfor\u001b[39;00m data_entry \u001b[39min\u001b[39;00m data_entries\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m--> 384\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(\n\u001b[0;32m    385\u001b[0m             surface,\n\u001b[0;32m    386\u001b[0m             data_entry,\n\u001b[0;32m    387\u001b[0m             pos_data,\n\u001b[0;32m    388\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos,\n\u001b[0;32m    389\u001b[0m             pos_ahead \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m    390\u001b[0m             Type\u001b[39m.\u001b[39;49mKNOWN,\n\u001b[0;32m    391\u001b[0m         )\n\u001b[0;32m    392\u001b[0m         any_matches \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    394\u001b[0m pos_ahead \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:275\u001b[0m, in \u001b[0;36mTokenizer.add\u001b[1;34m(self, surface, data_dict, from_pos_data, word_pos, end_pos, type_)\u001b[0m\n\u001b[0;32m    272\u001b[0m         least_idx \u001b[39m=\u001b[39m idx\n\u001b[0;32m    274\u001b[0m least_cost \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m word_cost\n\u001b[1;32m--> 275\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositions\u001b[39m.\u001b[39;49mget(end_pos)\u001b[39m.\u001b[39madd(\n\u001b[0;32m    276\u001b[0m     cost\u001b[39m=\u001b[39mleast_cost,\n\u001b[0;32m    277\u001b[0m     last_right_id\u001b[39m=\u001b[39mright_id,\n\u001b[0;32m    278\u001b[0m     back_pos\u001b[39m=\u001b[39mfrom_pos_data\u001b[39m.\u001b[39mpos,\n\u001b[0;32m    279\u001b[0m     back_rpos\u001b[39m=\u001b[39mword_pos,\n\u001b[0;32m    280\u001b[0m     back_index\u001b[39m=\u001b[39mleast_idx,\n\u001b[0;32m    281\u001b[0m     back_id\u001b[39m=\u001b[39mword_id,\n\u001b[0;32m    282\u001b[0m     back_dict_type\u001b[39m=\u001b[39mtype_,\n\u001b[0;32m    283\u001b[0m     back_pos_type\u001b[39m=\u001b[39mback_pos_type,\n\u001b[0;32m    284\u001b[0m     morphemes\u001b[39m=\u001b[39mmorphemes,\n\u001b[0;32m    285\u001b[0m     back_pos_tag\u001b[39m=\u001b[39mleft_pos,\n\u001b[0;32m    286\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:179\u001b[0m, in \u001b[0;36mTokenizer.WrappedPositionArray.get\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_positions \u001b[39m=\u001b[39m []\n\u001b[0;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount):\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_positions\u001b[39m.\u001b[39mappend(Tokenizer\u001b[39m.\u001b[39;49mPosition())\n\u001b[0;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_positions[\n\u001b[0;32m    182\u001b[0m     : \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_write\n\u001b[0;32m    183\u001b[0m ] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions[\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_write : \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_write\n\u001b[0;32m    185\u001b[0m ]\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_positions[\n\u001b[0;32m    187\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions) \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_write : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_write\n\u001b[0;32m    188\u001b[0m ] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositions[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_write]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\test01\\lib\\site-packages\\pecab\\_tokenizer.py:114\u001b[0m, in \u001b[0;36mTokenizer.Position.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_id \u001b[39m=\u001b[39m []\n\u001b[0;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_dict_type \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mback_pos_type \u001b[39m=\u001b[39m []\n\u001b[0;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmorphemes \u001b[39m=\u001b[39m []\n\u001b[0;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_pos_tag \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kss\n",
    "import re\n",
    "\n",
    "input_folder_path = \"D:\\\\video-summarization-master\\\\videos\\\\ssk_text_1\"\n",
    "output_folder_path = \"D:\\\\video-summarization-master\\\\videos\\\\ssk_text\"\n",
    "\n",
    "# 입력 폴더 내 모든 파일 확인\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    # .txt 파일인 경우만 처리\n",
    "    if filename.endswith(\".txt\"):\n",
    "        input_file_path = os.path.join(input_folder_path, filename)\n",
    "        output_file_path = os.path.join(output_folder_path, filename)\n",
    "\n",
    "        # 워드별 구분된 입력 파일을 문장별 구분된 출력 파일로 변환\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            content_lines = input_file.readlines()\n",
    "            word_intervals = []\n",
    "\n",
    "            # 각 단어의 시작 시간 기록\n",
    "            for line in content_lines:\n",
    "                word_start_time = float(line.split(' - 시작: ')[1].split('초')[0])\n",
    "                word = line.split(' - 시작: ')[0]\n",
    "                word_intervals.append((word, word_start_time))\n",
    "\n",
    "            words = ' '.join([interval[0] for interval in word_intervals])\n",
    "            text = words\n",
    "            sentences = kss.split_sentences(text)\n",
    "\n",
    "            if len(sentences) == 0:\n",
    "                continue\n",
    "\n",
    "            # 문장의 endIndex 찾기\n",
    "            start_index = 0\n",
    "            for sentence in sentences[:-1]:\n",
    "                sentence_start_time = word_intervals[start_index][1]\n",
    "                endIndex = start_index + len(sentence.split()) - 1\n",
    "                if endIndex >= len(word_intervals) - 1:\n",
    "                    break\n",
    "\n",
    "                # 문장 시작 타임스탬프를 설정하고 끝 인덱스를 업데이트합니다.\n",
    "                sentence_end_time = word_intervals[endIndex][1]\n",
    "                start_index = endIndex + 1\n",
    "\n",
    "                output_line = f\"{sentence}, 시작 시간:{sentence_start_time:.1f}초, 종료 시간:{sentence_end_time:.1f}초\"\n",
    "                output_file.write(output_line + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
